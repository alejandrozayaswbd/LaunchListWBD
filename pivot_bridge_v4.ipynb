{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e693876f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content Pipeline\n",
      "GR/TR/WAVE 3 Priority\n",
      "Max BRAND\n",
      "üèÖ EMEA Medal\n",
      "Primary Genre 1 üé≠\n",
      "Primary Genre 2 üé≠\n",
      "Sub-Genre 1 üé≠\n",
      "Sub-Genre 2 üé≠\n",
      "MAX Secondary Brand/s\n",
      "üë™ Kids Age Demo\n",
      "üë©‚Äçüë©‚Äçüëß‚Äçüëß Kids Content Privacy\n",
      "üîí License end\n",
      "Region\n",
      "üíØTOP 100 TITLE - LAUNCH\n",
      "Transition to Max Status\n",
      "üîíCountry of Origin\n",
      "Greece Converged Scheduling Priority\n",
      "‚úèÔ∏è Planned Premiere Date\n",
      "üîí Production Year\n",
      "üîí Distributor\n",
      "‚úÖ Actualizando: \"A | P&S ADR Secondary Max Brand_x\" ‚Üê \"A | P&S ADR Secondary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S CYP Secondary Max Brand_x\" ‚Üê \"A | P&S CYP Secondary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S CIS Secondary Max Brand_x\" ‚Üê \"A | P&S CIS Secondary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S GAL Secondary Max Brand\" ‚Üê \"A | P&S GAU Secondary Max Brand\"\n",
      "‚úÖ Actualizando: \"A | P&S GRE Secondary Max Brand_x\" ‚Üê \"A | P&S GRE Secondary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S ICE Secondary Max Brand_x\" ‚Üê \"A | P&S ICE Secondary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S IRE Secondary Max Brand_x\" ‚Üê \"A | P&S IRE Secondary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S ISR Secondary Max Brand_x\" ‚Üê \"A | P&S ISR Secondary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S ITA Secondary Max Brand_x\" ‚Üê \"A | P&S ITA Secondary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S MAL Secondary Max Brand_x\" ‚Üê \"A | P&S MAL Secondary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S UKR Secondary Max Brand_x\" ‚Üê \"A | P&S UKR Secondary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S UK Secondary Max Brand_x\" ‚Üê \"A | P&S UK Secondary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S LUX Secondary Max Brand_x\" ‚Üê \"A | P&S LUX Secondary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S SWI Secondary Max Brand\" ‚Üê \"A | P&S SWL Secondary Max Brand\"\n",
      "‚úÖ Actualizando: \"A | P&S BAL Secondary Max Brand copy\" ‚Üê \"A | P&S BAL Secondary Max Brand\"\n",
      "‚úÖ Actualizando: \"A | P&S ADR Primary Max Brand_x\" ‚Üê \"A | P&S ADR Primary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S CYP Primary Max Brand_x\" ‚Üê \"A | P&S CYP Primary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S CIS Primary Max Brand_x\" ‚Üê \"A | P&S CIS Primary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S GAL Primary Max Brand\" ‚Üê \"A | P&S GAU Primary Max Brand\"\n",
      "‚úÖ Actualizando: \"A | P&S GRE Primary Max Brand_x\" ‚Üê \"A | P&S GRE Primary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S ICE Primary Max Brand_x\" ‚Üê \"A | P&S ICE Primary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S IRE Primary Max Brand_x\" ‚Üê \"A | P&S IRE Primary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S ISR Primary Max Brand_x\" ‚Üê \"A | P&S ISR Primary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S ITA Primary Max Brand_x\" ‚Üê \"A | P&S ITA Primary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S MAL Primary Max Brand_x\" ‚Üê \"A | P&S MAL Primary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S UKR Primary Max Brand_x\" ‚Üê \"A | P&S UKR Primary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S UK Primary Max Brand_x\" ‚Üê \"A | P&S UK Primary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S LUX Primary Max Brand_x\" ‚Üê \"A | P&S LUX Primary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S SWL Primary Max Brand_x\" ‚Üê \"A | P&S SWL Primary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S BAL Primary Max Brand_x\" ‚Üê \"A | P&S BAL Primary Max Brand_y\"\n",
      "‚úÖ Actualizando: \"A | P&S ADR End Date_x\" ‚Üê \"A | P&S ADR End Date_y\"\n",
      "‚úÖ Actualizando: \"A | P&S CYP End Date_x\" ‚Üê \"A | P&S CYP End Date_y\"\n",
      "‚úÖ Actualizando: \"A | P&S CIS End Date_x\" ‚Üê \"A | P&S CIS End Date_y\"\n",
      "‚è≠Ô∏è  Saltando A | P&S GAL End Date_x (no existe en df)\n",
      "‚úÖ Actualizando: \"A | P&S GRE End Date_x\" ‚Üê \"A | P&S GRE End Date_y\"\n",
      "‚úÖ Actualizando: \"A | P&S ICE End Date_x\" ‚Üê \"A | P&S ICE End Date_y\"\n",
      "‚úÖ Actualizando: \"A | P&S IRE End Date_x\" ‚Üê \"A | P&S IRE End Date_y\"\n",
      "‚úÖ Actualizando: \"A | P&S ISR End Date_x\" ‚Üê \"A | P&S ISR End Date_y\"\n",
      "‚úÖ Actualizando: \"A | P&S Italy End Date\" ‚Üê \"A | P&S ITA End Date\"\n",
      "‚úÖ Actualizando: \"A | P&S MAL End Date_x\" ‚Üê \"A | P&S MAL End Date_y\"\n",
      "‚úÖ Actualizando: \"A | P&S UKR End Date_x\" ‚Üê \"A | P&S UKR End Date_y\"\n",
      "‚úÖ Actualizando: \"A | P&S UK End Date_x\" ‚Üê \"A | P&S UK End Date_y\"\n",
      "‚úÖ Actualizando: \"A | P&S LUX End Date_x\" ‚Üê \"A | P&S LUX End Date_y\"\n",
      "‚úÖ Actualizando: \"A | P&S SWL End Date_x\" ‚Üê \"A | P&S SWL End Date_y\"\n",
      "‚úÖ Actualizando: \"A | P&S GAU End Date_x\" ‚Üê \"A | P&S GAU End Date_y\"\n",
      "‚úÖ Actualizando: \"A | P&S BAL End Date_x\" ‚Üê \"A | P&S BAL End Date_y\"\n",
      "Procesando P&S | Title+Season ‚Üê A | Title + Season #\n",
      "Procesando P&S | Content Pipeline Summary ‚Üê Content Pipeline\n",
      "Procesando P&S | Primary Genre 1 Summary ‚Üê Primary Genre 1 üé≠\n",
      "Procesando P&S | Primary Genre 2 Summary ‚Üê Primary Genre 2 üé≠\n",
      "Procesando A | P&S Launch List Unique ID ‚Üê A | P&S Launch List Unique ID\n",
      "Procesando P&S | Kids Content Privacy Summary ‚Üê üë©‚Äçüë©‚Äçüëß‚Äçüëß Kids Content Privacy\n",
      "Procesando P&S | Kids Age Demo Summary ‚Üê üë™ Kids Age Demo\n",
      "Procesando P&S | Priority Regions Summary ‚Üê GR/TR/WAVE 3 Priority\n",
      "Procesando P&S | Sub-Genre 1 Summary ‚Üê Sub-Genre 1 üé≠\n",
      "Procesando P&S | Sub-Genre 2 Summary ‚Üê Sub-Genre 2 üé≠\n",
      "Procesando A | P&S Primary Max Brand ‚Üê Max BRAND\n",
      "Procesando A | P&S EMEA Medal ‚Üê üèÖ EMEA Medal\n",
      "Procesando A | Top 100 Regions Changes Status - Summary ‚Üê Resultado\n",
      "Procesando P&S | Launch List Validation ALL Regions ‚Üê Transition to Max Status\n",
      "Procesando P&S | Top 100 Regions Summary ‚Üê üíØTOP 100 TITLE - LAUNCH\n",
      "Procesando P&S | Greece Converged Scheduling Priority ‚Üê Greece Converged Scheduling Priority\n",
      "Procesando P&S | Distributor ‚Üê üîí Distributor\n",
      "Proceso completado ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings \n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_column\", None)\n",
    "\n",
    "# ========================\n",
    "# Funciones\n",
    "# ========================\n",
    "\n",
    "cols_map_secondary = {\n",
    "    \"A | P&S ADR Secondary Max Brand_x\": \"A | P&S ADR Secondary Max Brand_y\",\n",
    "    \"A | P&S CYP Secondary Max Brand_x\": \"A | P&S CYP Secondary Max Brand_y\",\n",
    "    \"A | P&S CIS Secondary Max Brand_x\": \"A | P&S CIS Secondary Max Brand_y\",\n",
    "    \"A | P&S GAL Secondary Max Brand\": \"A | P&S GAU Secondary Max Brand\",\n",
    "    \"A | P&S GRE Secondary Max Brand_x\": \"A | P&S GRE Secondary Max Brand_y\",\n",
    "    \"A | P&S ICE Secondary Max Brand_x\": \"A | P&S ICE Secondary Max Brand_y\",\n",
    "    \"A | P&S IRE Secondary Max Brand_x\": \"A | P&S IRE Secondary Max Brand_y\",\n",
    "    \"A | P&S ISR Secondary Max Brand_x\": \"A | P&S ISR Secondary Max Brand_y\",\n",
    "    \"A | P&S ITA Secondary Max Brand_x\": \"A | P&S ITA Secondary Max Brand_y\",\n",
    "    \"A | P&S MAL Secondary Max Brand_x\": \"A | P&S MAL Secondary Max Brand_y\",\n",
    "    \"A | P&S UKR Secondary Max Brand_x\": \"A | P&S UKR Secondary Max Brand_y\",\n",
    "    \"A | P&S UK Secondary Max Brand_x\": \"A | P&S UK Secondary Max Brand_y\",\n",
    "    \"A | P&S LUX Secondary Max Brand_x\": \"A | P&S LUX Secondary Max Brand_y\",\n",
    "    \"A | P&S SWI Secondary Max Brand\": \"A | P&S SWL Secondary Max Brand\",\n",
    "    \"A | P&S BAL Secondary Max Brand copy\":\"A | P&S BAL Secondary Max Brand\"\n",
    "    \n",
    "}\n",
    "\n",
    "cols_map_primary = {\n",
    "    \"A | P&S ADR Primary Max Brand_x\": \"A | P&S ADR Primary Max Brand_y\",\n",
    "    \"A | P&S CYP Primary Max Brand_x\": \"A | P&S CYP Primary Max Brand_y\",\n",
    "    \"A | P&S CIS Primary Max Brand_x\": \"A | P&S CIS Primary Max Brand_y\",\n",
    "    \"A | P&S GAL Primary Max Brand\": \"A | P&S GAU Primary Max Brand\",\n",
    "    \"A | P&S GRE Primary Max Brand_x\": \"A | P&S GRE Primary Max Brand_y\",\n",
    "    \"A | P&S ICE Primary Max Brand_x\": \"A | P&S ICE Primary Max Brand_y\",\n",
    "    \"A | P&S IRE Primary Max Brand_x\": \"A | P&S IRE Primary Max Brand_y\",\n",
    "    \"A | P&S ISR Primary Max Brand_x\": \"A | P&S ISR Primary Max Brand_y\",\n",
    "    \"A | P&S ITA Primary Max Brand_x\": \"A | P&S ITA Primary Max Brand_y\",\n",
    "    \"A | P&S MAL Primary Max Brand_x\": \"A | P&S MAL Primary Max Brand_y\",\n",
    "    \"A | P&S UKR Primary Max Brand_x\": \"A | P&S UKR Primary Max Brand_y\",\n",
    "    \"A | P&S UK Primary Max Brand_x\": \"A | P&S UK Primary Max Brand_y\",\n",
    "    \"A | P&S LUX Primary Max Brand_x\": \"A | P&S LUX Primary Max Brand_y\",\n",
    "    \"A | P&S SWL Primary Max Brand_x\": \"A | P&S SWL Primary Max Brand_y\",\n",
    "    \"A | P&S BAL Primary Max Brand_x\": \"A | P&S BAL Primary Max Brand_y\"\n",
    "}\n",
    "\n",
    "cols_map_end_date = {\n",
    "    \"A | P&S ADR End Date_x\":\"A | P&S ADR End Date_y\",\n",
    "    \"A | P&S CYP End Date_x\":\"A | P&S CYP End Date_y\",\n",
    "    \"A | P&S CIS End Date_x\":\"A | P&S CIS End Date_y\",\n",
    "    \"A | P&S GAL End Date_x\":\"A | P&S GAL End Date_y\",\n",
    "    \"A | P&S GRE End Date_x\":\"A | P&S GRE End Date_y\",\n",
    "    \"A | P&S ICE End Date_x\":\"A | P&S ICE End Date_y\",\n",
    "    \"A | P&S IRE End Date_x\":\"A | P&S IRE End Date_y\",\n",
    "    \"A | P&S ISR End Date_x\":\"A | P&S ISR End Date_y\",\n",
    "    \"A | P&S Italy End Date\":\"A | P&S ITA End Date\",\n",
    "    \"A | P&S MAL End Date_x\":\"A | P&S MAL End Date_y\",\n",
    "    \"A | P&S UKR End Date_x\":\"A | P&S UKR End Date_y\",\n",
    "    \"A | P&S UK End Date_x\":\"A | P&S UK End Date_y\",\n",
    "    \"A | P&S LUX End Date_x\":\"A | P&S LUX End Date_y\",\n",
    "    \"A | P&S SWL End Date_x\":\"A | P&S SWL End Date_y\",\n",
    "    \"A | P&S GAU End Date_x\":\"A | P&S GAU End Date_y\",\n",
    "    \"A | P&S BAL End Date_x\":\"A | P&S BAL End Date_y\"\n",
    "}\n",
    "\n",
    "\n",
    "today = datetime.today().strftime(\"%b%d\").upper()\n",
    "\n",
    "def split_region_string(s: str):\n",
    "    \"\"\"\n",
    "    Convierte una cadena gigante de regiones + descripciones\n",
    "    en una lista donde cada elemento corresponde a una regi√≥n.\n",
    "    \n",
    "    Args:\n",
    "        s (str): Cadena de entrada\n",
    "    \n",
    "    Returns:\n",
    "        list[str]: Lista de strings separados por regi√≥n\n",
    "    \"\"\"\n",
    "    # Paso 1: separar por comas\n",
    "    parts = [p.strip() for p in s.split(\",\")]\n",
    "\n",
    "    result = []\n",
    "    buffer = []\n",
    "\n",
    "    # Paso 2: identificar el inicio de un bloque (ej. \"ADR - ...\")\n",
    "    for p in parts:\n",
    "        if re.match(r\"^[A-Z]{2,3}\\s-\\s\", p):  # detecta inicio de bloque\n",
    "            # si ya hab√≠a algo acumulado, lo unimos y guardamos\n",
    "            if buffer:\n",
    "                result.append(\",\".join(buffer))\n",
    "                buffer = []\n",
    "            buffer.append(p)  # nuevo bloque\n",
    "        else:\n",
    "            buffer.append(p)  # continuaci√≥n del bloque\n",
    "\n",
    "    # √∫ltimo bloque\n",
    "    if buffer:\n",
    "        result.append(\",\".join(buffer))\n",
    "        \n",
    "    result = [f\"\\\"{x}\\\"\" for x in result]\n",
    "\n",
    "    return result\n",
    "\n",
    "def compare_history_vs_snapshot(row):\n",
    "    history = row['A | Top 100 Regions Changes Status - Summary']\n",
    "    snapshot = row['üíØTOP 100 TITLE - LAUNCH']\n",
    "    \n",
    "    # Convertir en listas\n",
    "    hist_entries = [h.strip() for h in history.split(\",\") if h.strip() != \"\"]\n",
    "    snap_entries = [s.strip().replace(\" - checked\",\"\") for s in snapshot.split(\",\") if s.strip() != \"\"]\n",
    "    \n",
    "    # Filtrar ruido: descartar los que empiezan con \"-\" o cuyo c√≥digo es \"-\"\n",
    "    clean_hist_entries = []\n",
    "    for h in hist_entries:\n",
    "        if \" - \" in h:\n",
    "            code, action = h.split(\" - \", 1)\n",
    "            if code.strip() != \"-\":\n",
    "                clean_hist_entries.append(h)\n",
    "    \n",
    "    # Extraer c√≥digos del historial ya limpio\n",
    "    hist_codes = [h.split(\" - \")[0] for h in clean_hist_entries if \" - \" in h]\n",
    "    \n",
    "    # Guardamos los cambios en una lista separada\n",
    "    changes = []\n",
    "    for code in set(snap_entries) | set(hist_codes):\n",
    "        if code in snap_entries and code in hist_codes:\n",
    "            changes.append(f\"{code} - Maintained {today}\")\n",
    "        elif code in snap_entries and code not in hist_codes:\n",
    "            changes.append(f\"{code} - Added {today}\")\n",
    "        elif code not in snap_entries and code in hist_codes:\n",
    "            changes.append(f\"{code} - Removed {today}\")\n",
    "    \n",
    "    # Ahora historial primero, luego cambios\n",
    "    results = changes + clean_hist_entries\n",
    "    results = [r for r in (changes + clean_hist_entries) if not r.startswith(\"- \")]\n",
    "\n",
    "    \n",
    "    return \",\".join(results)\n",
    "\n",
    "def compare_history_vs_snapshot_min(row): \n",
    "    history = row['A | Season/Feature Validation Comments AV'] \n",
    "    snapshot = row['A | Season Active'] \n",
    "    # Los convertimos a lista \n",
    "    hist_entries = [h.strip() for h in history.split(\",\") if h.strip() !=\"\"] \n",
    "    snap_entries = [s.strip() for s in snapshot.split(\",\") if s.strip()!=\"\"] \n",
    "    changes = [] \n",
    "    \n",
    "    for snap in snap_entries: \n",
    "        if snap.__contains__(\"No Transition\"): \n",
    "            changes.append(f\"{today} - Removed\") \n",
    "        else: \n",
    "            changes.append(F\"{today} - {snap}\") \n",
    "    \n",
    "    results = changes + hist_entries \n",
    "    \n",
    "    return \" | \".join(results)\n",
    "\n",
    "def expand_end_dates(df, col=\"Code + üîí License end\"):\n",
    "    \"\"\"\n",
    "    Expande columna de fechas en columnas separadas conservando la original.\n",
    "    \"\"\"\n",
    "    def extract_dates(text):\n",
    "        if pd.isna(text) or text.strip() == \"-\":\n",
    "            return {}\n",
    "        codes_dict = {}\n",
    "        for block in text.split(\",\"):\n",
    "            block = block.strip()\n",
    "            if \" - \" in block:\n",
    "                code, value = block.split(\" - \", 1)\n",
    "                codes_dict[f\"A | P&S {code.strip()} {col}\"] = value.strip()\n",
    "        return codes_dict\n",
    "\n",
    "    expanded = df[col].apply(extract_dates).apply(pd.Series)\n",
    "    df = pd.concat([df, expanded], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def expand_secondary_brands(df, col=\"Code + MAX Secondary Brand/s\"):\n",
    "    \"\"\"\n",
    "    Expande columna de c√≥digos de regi√≥n y marcas en columnas separadas.\n",
    "    Cambia el nombre de la columna:\n",
    "        Max BRAND -> Primary Max Brand\n",
    "        MAX Secondary Brand/s -> Secondary Max Brand\n",
    "    \"\"\"\n",
    "    def extract_codes(text):\n",
    "        if pd.isna(text) or text.strip() == \"-\":\n",
    "            return {}\n",
    "        text = text.strip().strip('\"')\n",
    "        codes_dict = {}\n",
    "        for block in text.split(\",\"):\n",
    "            block = block.strip().strip('\"')\n",
    "            if \" - \" in block:\n",
    "                code, value = block.split(\" - \", 1)\n",
    "                code = code.strip()\n",
    "\n",
    "                # üîß Normalizar c√≥digos\n",
    "                if code == \"SWL\":\n",
    "                    code = \"SWL\"\n",
    "\n",
    "                # Determinar el nombre final\n",
    "                base_name = col.replace(\"Code + \", \"\")\n",
    "                if base_name == \"Max BRAND\":\n",
    "                    base_name = \"Primary Max Brand\"\n",
    "                elif base_name == \"MAX Secondary Brand/s\":\n",
    "                    base_name = \"Secondary Max Brand\"\n",
    "\n",
    "                codes_dict[f\"A | P&S {code} {base_name}\"] = value.strip()\n",
    "        return codes_dict\n",
    "\n",
    "    expanded = df[col].apply(extract_codes).apply(pd.Series)\n",
    "    df = pd.concat([df, expanded], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def clean_priority_list(series: pd.Series, col_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia y ordena listas de regi√≥n-prioridad.\n",
    "    \"\"\"\n",
    "    valid_entries = series.dropna().astype(str).str.strip()\n",
    "    \n",
    "    filtered = [entry for entry in valid_entries if not entry.endswith(\" - -\")]\n",
    "\n",
    "    return \",\".join(sorted(filtered))\n",
    "\n",
    "\n",
    "def split_region(df: pd.DataFrame, region_name: str, codes: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Separa filas con m√∫ltiples regiones en varias filas.\n",
    "    \"\"\"\n",
    "    rows = df[df[\"Region\"] == region_name]\n",
    "    if rows.empty:\n",
    "        return df\n",
    "    df = df[df[\"Region\"] != region_name]\n",
    "    new_rows = [rows.assign(**{\"Region Code\": code}) for code in codes]\n",
    "    return pd.concat([df] + new_rows, ignore_index=True)\n",
    "\n",
    "\n",
    "def update_from_map(df: pd.DataFrame, cols_map: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Actualiza columnas *_x con los valores de *_y seg√∫n un diccionario de mapeo.\n",
    "    - Si la columna *_y no existe, conserva *_x sin cambios.\n",
    "    - Si la columna *_x no existe, se salta.\n",
    "    \"\"\"\n",
    "    for col_x, col_y in cols_map.items():\n",
    "        if col_x in df.columns:\n",
    "            if col_y in df.columns:\n",
    "                print(f\"‚úÖ Actualizando: \\\"{col_x}\\\" ‚Üê \\\"{col_y}\\\"\")\n",
    "            \n",
    "                mask = df[col_y].notnull() & (df[col_x].isnull() | (df[col_x] != df[col_y]))\n",
    "                df.loc[mask, col_x] = df.loc[mask, col_y]\n",
    "                df.drop(columns=[col_y], inplace=True)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  {col_y} no existe ‚Üí se conserva \\\"{col_x}\\\" tal cual\")\n",
    "        else:\n",
    "            print(f\"‚è≠Ô∏è  Saltando {col_x} (no existe en df)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_unique_value(pattern: str) -> str:\n",
    "    # Separar por coma\n",
    "    parts = [p.strip() for p in pattern.split(\",\")]\n",
    "    # Quitar duplicados manteniendo orden\n",
    "    unique_parts = list(dict.fromkeys(parts))\n",
    "    # Si hay m√°s de un valor, quedarnos con el √∫ltimo distinto de \"-\"\n",
    "    for val in reversed(unique_parts):\n",
    "        if val != \"-\":\n",
    "            return val\n",
    "    # Si todos son \"-\", devolver \"-\"\n",
    "    return \"-\"\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Paths y carga de data\n",
    "# ========================\n",
    "\n",
    "bridge_path = r\"C:\\Users\\azayas\\Downloads\\W3 Launch List (Rebecca) Last One as exc.xlsx\"\n",
    "main_path = r\"C:\\Users\\azayas\\Downloads\\P&S Launch List (Pivot)-EMEA W3 Launch List - Ally (35).csv\"\n",
    "\n",
    "# \n",
    "output_path = Path(r\"C:\\Temp\\EMEA\\Test Cases\")\n",
    "\n",
    "bridge_df = pd.read_excel(bridge_path, engine=\"openpyxl\")\n",
    "main_df = pd.read_csv(main_path)\n",
    "\n",
    "# ========================\n",
    "# Tratamiento de fechas\n",
    "# ========================\n",
    "\n",
    "bridge_df['üîí License end'] = pd.to_datetime(bridge_df['üîí License end'], format=\"%d-%b-%y\", errors=\"coerce\")\n",
    "\n",
    "\n",
    "bridge_df['üîí License end'] = bridge_df['üîí License end'].dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "bridge_df['üîí License end'] = bridge_df['üîí License end'].astype(str)\n",
    "\n",
    "bridge_df['‚úèÔ∏è Planned Premiere Date'] = pd.to_datetime(bridge_df['‚úèÔ∏è Planned Premiere Date'], format=\"%d-%b-%y\", errors=\"coerce\")\n",
    "\n",
    "\n",
    "bridge_df['‚úèÔ∏è Planned Premiere Date'] = bridge_df['‚úèÔ∏è Planned Premiere Date'].dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "bridge_df['‚úèÔ∏è Planned Premiere Date'] = bridge_df['‚úèÔ∏è Planned Premiere Date'].astype(str)\n",
    "\n",
    "\n",
    "bridge_df['üë™ Kids Age Demo'] = bridge_df['üë™ Kids Age Demo'].str.replace(\",\", \"|\", regex=False)\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Renombrar columnas y mapear regiones\n",
    "# ========================\n",
    "cols_map = {\"üîí Region\": \"Region\"} \n",
    "# cols_map = {\"üîí Region\": \"Region\", \"GR/TR/WAVE 3 Priority\": \"Priority\"}\n",
    "bridge_df.rename(columns=cols_map, inplace=True)\n",
    "\n",
    "region_code_map = {\n",
    "    \"Adria\": \"ADR\", \"Iceland\": \"ICE\", \"Baltics\": \"BAL\", \"CIS\": \"CIS\",\n",
    "    \"Israel\": \"ISR\", \"Malta\": \"MAL\", \"Ukraine\": \"UKR\", \"Ireland\": \"IRE\",\n",
    "    \"UK\": \"UK\", \"Germany\": \"GAU\", \"Italy\": \"ITA\", \"Luxembourg\": \"LUX\",\n",
    "    \"Switzerland\": \"SWL\",\n",
    "    \"Greece & Cyprus\": None, \"UK & Ireland\": None\n",
    "}\n",
    "\n",
    "df = bridge_df.copy()\n",
    "df[\"Region\"] = df[\"Region\"].str.strip()\n",
    "df[\"GR/TR/WAVE 3 Priority\"] = df[\"GR/TR/WAVE 3 Priority\"].fillna(\"-\").astype(str).str.strip()\n",
    "df[\"Region Code\"] = df[\"Region\"].map(region_code_map.get)\n",
    "\n",
    "df['üîí Production Year'] = df['üîí Production Year'].astype(\"Int64\").astype(str)\n",
    "\n",
    "#df['üîí Production Year'] = df['üîí Production Year'].fillna(\"-\").astype(str).str.strip()\n",
    "\n",
    "# ========================\n",
    "# Tratamiento para Greece\n",
    "# ========================\n",
    "\n",
    "## Llenamos los vac√≠os con \"-\"\n",
    "df['Greece Converged Scheduling Priority'] = df['Greece Converged Scheduling Priority'].fillna(\"-\")\n",
    "df['Greece Converged Scheduling Priority'] = df['Greece Converged Scheduling Priority'].astype(str)\n",
    "df['Greece Vodafone Status'] = df['Greece Vodafone Status'].fillna(\"-\")\n",
    "\n",
    "\n",
    "# Separar regiones especiales\n",
    "region_expansions = {\"Greece & Cyprus\": [\"GRE\", \"CYP\"], \"UK & Ireland\": [\"UK\",\"IRE\"]}\n",
    "for region, codes in region_expansions.items():\n",
    "    df = split_region(df, region, codes)   \n",
    "\n",
    "# ========================\n",
    "# Tratamiento para Transiction to Max\n",
    "# ========================\n",
    "df['Transition to Max Status'] = df['Transition to Max Status'].fillna(\"-\")\n",
    "\n",
    "df['Transition to Max Status'] = df['Transition to Max Status'].replace({\"DAY 1 - Launch Catalogue\":\"For Launch\", \"-\":\"No Transition to Max\"})\n",
    "\n",
    "df['Transition to Max Status'] = np.where(df['Transition to Max Status'].str.contains(\"Day\", na=False), \"For Launch\", df['Transition to Max Status'])\n",
    "df['Transition to Max Status'] = np.where(df['Transition to Max Status'].str.contains(\"Remove\", na=False), \"No Transition to Max\", df['Transition to Max Status'])\n",
    "\n",
    "# ========================\n",
    "# Preparar columnas de c√≥digo\n",
    "# ========================\n",
    "\n",
    "same_cols = [\n",
    "    \"Content Pipeline\", \"GR/TR/WAVE 3 Priority\", \"Max BRAND\", \"üèÖ EMEA Medal\",\n",
    "    \"Primary Genre 1 üé≠\", \"Primary Genre 2 üé≠\", \"Sub-Genre 1 üé≠\", \"Sub-Genre 2 üé≠\",\n",
    "    \"MAX Secondary Brand/s\", \"üë™ Kids Age Demo\", \"üë©‚Äçüë©‚Äçüëß‚Äçüëß Kids Content Privacy\", \"üîí License end\",\n",
    "    \"Region\",\"üíØTOP 100 TITLE - LAUNCH\", \"Transition to Max Status\",\"üîíCountry of Origin\", \"Greece Converged Scheduling Priority\",'‚úèÔ∏è Planned Premiere Date','üîí Production Year',\"üîí Distributor\"\n",
    "    #,\"Greece Vodafone Status\",\n",
    "]\n",
    "\n",
    "cols_main = [\n",
    "    \"A | P&S Launch List Unique ID\", \"P&S | Title+Season\", \"P&S | Content Pipeline Summary\",\n",
    "    \"P&S | Priority Regions Summary\", \"A | P&S Primary Max Brand\", \"A | P&S EMEA Medal\",\n",
    "    \"P&S | Primary Genre 1 Summary\", \"P&S | Primary Genre 2 Summary\",\n",
    "    \"P&S | Sub-Genre 1 Summary\", \"P&S | Sub-Genre 2 Summary\", \"A | P&S Secondary Max Brand\",\n",
    "    \"P&S | Kids Age Demo Summary\", \"P&S | Kids Content Privacy Summary\", \n",
    "]\n",
    "\n",
    "\n",
    "for col in same_cols:\n",
    "    print(col)\n",
    "    \n",
    "    if col == \"Region\":\n",
    "        #print(f\"Concatenando columna: {col}\")\n",
    "        df[f\"Code + {col}\"] = df[\"Region Code\"]\n",
    "    elif col == \"Greece Converged Scheduling Priority\":\n",
    "        df[f\"Code + {col}\"] = df[col]\n",
    "        \n",
    "    elif col == \"Greece Vodafone Status\":\n",
    "        print(f\"Greece: {col}\")\n",
    "        df[f\"Code + {col}\"] = df[col]\n",
    "        \n",
    "    elif col == \"üîí Distributor\":\n",
    "        df[f\"Code + {col}\"] = df[col]\n",
    "    else:\n",
    "        #print(f\"Concatenando columna: {col}\")\n",
    "        df[f\"Code + {col}\"] = df[\"Region Code\"] + \" - \" + df[col]\n",
    "\n",
    "# ========================\n",
    "# Agregar data agrupada\n",
    "# ========================\n",
    "\n",
    "result = df[[\"A | P&S Launch List Unique ID\", \"A | Title + Season #\"]].drop_duplicates(\n",
    "    subset=[\"A | P&S Launch List Unique ID\"]\n",
    ")\n",
    "\n",
    "for new_col in [f\"Code + {c}\" for c in same_cols]:\n",
    "    grouped = (\n",
    "        df.groupby(\"A | P&S Launch List Unique ID\")[new_col]\n",
    "        .apply(lambda x: clean_priority_list(x, col_name=new_col))\n",
    "        .reset_index()\n",
    "    )\n",
    "    grouped[new_col] = grouped[new_col].replace(\"\", \"-\").fillna(\"-\")\n",
    "    result = result.merge(grouped, on=\"A | P&S Launch List Unique ID\", how=\"left\")\n",
    "\n",
    "result = result.loc[:, ~result.columns.duplicated(keep=\"last\")]\n",
    "\n",
    "# ========================\n",
    "# Expandir Secondary Max Brand y End Dates\n",
    "# ========================\n",
    "\n",
    "\n",
    "result = expand_secondary_brands(result, col=\"Code + MAX Secondary Brand/s\")\n",
    "\n",
    "result = expand_secondary_brands(result, col=\"Code + Max BRAND\")\n",
    "\n",
    "result.rename(columns={\"Code + üîí License end\":\"End Date\"}, inplace=True)\n",
    "\n",
    "result = expand_end_dates(result, col=\"End Date\")\n",
    "\n",
    "\n",
    "result.rename(columns={\"Code + ‚úèÔ∏è Planned Premiere Date\":\"‚úèÔ∏è Planned Premiere Date\"}, inplace=True)\n",
    "\n",
    "result = expand_end_dates(result, col=\"‚úèÔ∏è Planned Premiere Date\")\n",
    "\n",
    "uk_planned_premiere_name = [c for c in result if c=='A | P&S UK ‚úèÔ∏è Planned Premiere Date']\n",
    "\n",
    "columns_planned_premiere_name = [c for c in result if (\"‚úèÔ∏è Planned Premiere Date\" in c)&(c not in uk_planned_premiere_name)&(c !='‚úèÔ∏è Planned Premiere Date')]\n",
    "\n",
    "result.rename(columns=dict(zip(columns_planned_premiere_name,[x[:11] + x[14:] for x in columns_planned_premiere_name])), inplace=True)\n",
    "  \n",
    "## Cambiamos solo para UK\n",
    "\n",
    "result.rename(columns={'A | P&S UK ‚úèÔ∏è Planned Premiere Date':'A | P&S UK Planned Premiere Date'}, inplace=True)\n",
    "\n",
    "#result = expand_end_dates(result, col=\"Code + P&S | Planned Premiere Date\")\n",
    "\n",
    "## Regresamos las columnas al nombre original del bridge\n",
    "\n",
    "bridge_cols_map = {f\"Code + {c}\":c for c in same_cols}\n",
    "result.rename(mapper=bridge_cols_map, axis=1, inplace=True)\n",
    "\n",
    "# ========================\n",
    "# Merge con Main y actualizar columnas din√°micamente\n",
    "# ========================\n",
    "\n",
    "merged = main_df.merge(result, on=\"A | P&S Launch List Unique ID\", how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "merged = update_from_map(merged, cols_map_secondary)\n",
    "merged = update_from_map(merged, cols_map_primary)\n",
    "\n",
    "\n",
    "merged = update_from_map(merged, cols_map_end_date)\n",
    "\n",
    "\n",
    "## Tratamos las columnas Greece Converged Scheduling Priority\ty Greece Vodafone Status\n",
    "\n",
    "merged[\"Greece Converged Scheduling Priority\"] = merged[\"Greece Converged Scheduling Priority\"].fillna(\"-\")\n",
    "#merged['Greece Vodafone Status'] = merged['Greece Vodafone Status'].fillna(\"-\")\n",
    "\n",
    "merged['Greece Converged Scheduling Priority'] = merged['Greece Converged Scheduling Priority'].apply(get_unique_value)\n",
    "\n",
    "merged['üîí Distributor'] = merged['üîí Distributor'].fillna(\"-\")\n",
    "\n",
    "merged['üîí Distributor'] = merged['üîí Distributor'].apply(get_unique_value)\n",
    "#merged['Greece Vodafone Status'] = merged['Greece Vodafone Status'].apply(get_unique_value)\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Actualizamos las dem√°s columnas\n",
    "# ========================\n",
    "ordered_dict = {\n",
    "    \"P&S | Title+Season\": \"A | Title + Season #\",\n",
    "    \"P&S | Content Pipeline Summary\": \"Content Pipeline\",\n",
    "    \"P&S | Primary Genre 1 Summary\": \"Primary Genre 1 üé≠\",\n",
    "    \"P&S | Primary Genre 2 Summary\": \"Primary Genre 2 üé≠\",\n",
    "    \"A | P&S Launch List Unique ID\": \"A | P&S Launch List Unique ID\",\n",
    "    \"P&S | Kids Content Privacy Summary\": \"üë©‚Äçüë©‚Äçüëß‚Äçüëß Kids Content Privacy\",\n",
    "    \"P&S | Kids Age Demo Summary\": \"üë™ Kids Age Demo\",\n",
    "    \"P&S | Priority Regions Summary\": \"GR/TR/WAVE 3 Priority\",\n",
    "    \"P&S | Sub-Genre 1 Summary\": \"Sub-Genre 1 üé≠\",\n",
    "    \"P&S | Sub-Genre 2 Summary\": \"Sub-Genre 2 üé≠\",\n",
    "    \"A | P&S Primary Max Brand\": \"Max BRAND\",\n",
    "    \"A | P&S EMEA Medal\": \"üèÖ EMEA Medal\",\n",
    "    \"A | Top 100 Regions Changes Status - Summary\":\"Resultado\",\n",
    "    \"P&S | Launch List Validation ALL Regions\":\"Transition to Max Status\",\n",
    "    \"P&S | Top 100 Regions Summary\":\"üíØTOP 100 TITLE - LAUNCH\",\n",
    "    \"P&S | Greece Converged Scheduling Priority\":\"Greece Converged Scheduling Priority\",\n",
    "    \"P&S | Distributor\":\"üîí Distributor\"\n",
    "    #\"P&S | Greece Vodafone Status\":\"Greece Vodafone Status\",\n",
    "    #\"P&S | Content Pipeline\": \"Content Pipeline\",\n",
    "}\n",
    "\n",
    "## Regresamos las columnas al nombre original del bridge\n",
    "result.rename(mapper=bridge_cols_map, axis=1, inplace=True)\n",
    "\n",
    "# ========================\n",
    "# Actualizamos las dem√°s columnas\n",
    "# ========================\n",
    "\n",
    "\n",
    "merged['üíØTOP 100 TITLE - LAUNCH'] = merged['üíØTOP 100 TITLE - LAUNCH'].fillna(\"-\")\n",
    "merged['A | Top 100 Regions Changes Status - Summary'] = merged['A | Top 100 Regions Changes Status - Summary'].fillna(\"-\")\n",
    "\n",
    "merged[\"Resultado\"] = merged.apply(compare_history_vs_snapshot, axis=1)\n",
    "\n",
    "merged['Resultado'] = merged['Resultado'].map(lambda x:x.replace(\"- - Addeed\", \"\"))\n",
    "\n",
    "merged['Resultado'].replace({f\"- - Added {today}\":\"-\"}, inplace=True)\n",
    "\n",
    "\n",
    "for main_col, bridge_col in ordered_dict.items():\n",
    "    print(f\"Procesando {main_col} ‚Üê {bridge_col}\")\n",
    "    mask_update = merged[bridge_col].notnull() & (\n",
    "        merged[main_col].isnull() | (merged[main_col] != merged[bridge_col])\n",
    "    )\n",
    "    merged.loc[mask_update, main_col] = merged.loc[mask_update, bridge_col]\n",
    "\n",
    "new_main_df = merged.copy()\n",
    "\n",
    "\n",
    "for col in [c for c in new_main_df.columns if \"End Date\" in c]:\n",
    "    new_main_df[col] = pd.to_datetime(new_main_df[col],format=\"%d/%m/%Y\", errors=\"coerce\").dt.date\n",
    "    \n",
    "for col in [c for c in new_main_df.columns if \"Premiere\" in c]:\n",
    "    new_main_df[col] = pd.to_datetime(new_main_df[col],format=\"%d/%m/%Y\", errors=\"coerce\").dt.date\n",
    "    \n",
    "#new_main_df.to_excel(output_path / \"updated_main_dates_fixed.xlsx\", index=False)\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Guardar archivos\n",
    "# ========================\n",
    "\n",
    "\n",
    "\n",
    "columnas_deseadas = list(dict.fromkeys(\n",
    "    list(ordered_dict.keys()) \n",
    "    + [c for c in new_main_df.columns if \"Secondary\" in c] \n",
    "    + [c for c in new_main_df.columns if \"Primary\" in c] \n",
    "    + [c for c in new_main_df.columns if \"End Date\" in c]\n",
    "    + [c for c in new_main_df.columns if \"Premiere\" in c]\n",
    "    +['üîí Production Year']\n",
    "    +['A | Season/Feature Validation Comments AV']\n",
    "))\n",
    "\n",
    "\n",
    "new_main_df_cols = new_main_df[columnas_deseadas].copy()\n",
    "\n",
    "for col in [c for c in new_main_df_cols.columns if \"Secondary Max Brand\" in c]:\n",
    "    \n",
    "    new_main_df_cols.loc[(new_main_df_cols[col] != \"-\")&(new_main_df_cols[col].notna()),col].map(lambda x: x.replace(\" - \",\",\"))\n",
    "    \n",
    "new_main_df_cols.columns = [x.replace(\"_x\",\"\") if x.endswith(\"_x\") else x for x in new_main_df_cols.columns]\n",
    "\n",
    "for col in [c for c in new_main_df_cols.columns if \"Secondary\" in c]:\n",
    "    mask = new_main_df_cols[col].notna() & (new_main_df_cols[col] != \"-\")\n",
    "    #new_main_df_cols.loc[mask, col] = new_main_df_cols.loc[mask, col].map(lambda x: f'\\\"{x}\\\"')\n",
    "    \n",
    "## Cambios repetidas \"P&S | Top 100 Regions Summary\"\n",
    "\n",
    "new_main_df_cols.loc[(new_main_df_cols[\"P&S | Top 100 Regions Summary\"].notna()),\"P&S | Top 100 Regions Summary\" ] = new_main_df_cols.loc[(new_main_df_cols[\"P&S | Top 100 Regions Summary\"].notna()),\"P&S | Top 100 Regions Summary\" ].map(lambda x: \",\".join(set(x.split(\",\"))))\n",
    "\n",
    "# Cambios repetidas \"A | P&S EMEA Medal\"\n",
    "\n",
    "new_main_df_cols.loc[(new_main_df_cols[\"A | P&S EMEA Medal\"].notna()), \"A | P&S EMEA Medal\"] = new_main_df_cols.loc[(new_main_df_cols[\"A | P&S EMEA Medal\"].notna()), \"A | P&S EMEA Medal\"].map(lambda x: \",\".join(set(x.split(\",\"))))\n",
    "\n",
    "aux = bridge_df[[\"üîíCountry of Origin\",\"A | P&S Launch List Unique ID\"]].copy()\n",
    "\n",
    "aux.drop_duplicates(subset=\"A | P&S Launch List Unique ID\", inplace=True)\n",
    "\n",
    "new_main_df_cols = new_main_df_cols.merge(aux, on=\"A | P&S Launch List Unique ID\", how=\"left\")\n",
    "\n",
    "new_main_df_cols.rename(columns={'üîíCountry of Origin':'A | P&S Country of Origin'}, inplace=True)\n",
    "\n",
    "new_main_df_cols.loc[(new_main_df_cols['P&S | Kids Age Demo Summary'].notna())&(new_main_df_cols['P&S | Kids Age Demo Summary']!=\"-\"), 'P&S | Kids Age Demo Summary'] = new_main_df_cols.loc[(new_main_df_cols['P&S | Kids Age Demo Summary'].notna())&(new_main_df_cols['P&S | Kids Age Demo Summary']!=\"-\"), 'P&S | Kids Age Demo Summary'].map(lambda x: \",\".join(split_region_string(x)))\n",
    "\n",
    "aux_top = bridge_df[['A | P&S Launch List Unique ID','üíØTOP 100 TITLE - LAUNCH',\"Region\"]].copy()\n",
    "\n",
    "aux_top = aux_top.loc[(aux_top['üíØTOP 100 TITLE - LAUNCH'].notna())&(aux_top['üíØTOP 100 TITLE - LAUNCH']==\"checked\")].copy()\n",
    "aux_top['Region Code'] = aux_top['Region'].map(region_code_map.get)\n",
    "\n",
    "# Separar regiones especiales\n",
    "region_expansions = {\"Greece & Cyprus\": [\"GRE\", \"CYP\"], \"UK & Ireland\": [\"UK\",\"IRE\"]}\n",
    "for region, codes in region_expansions.items():\n",
    "    aux_top = split_region(aux_top, region, codes)\n",
    "\n",
    "# Separar regiones especiales\n",
    "region_expansions = {\"Greece & Cyprus\": [\"GRE\", \"CYP\"], \"UK & Ireland\": [\"UK\",\"IRE\"]}\n",
    "for region, codes in region_expansions.items():\n",
    "    aux_top = split_region(aux_top, region, codes)\n",
    "\n",
    "aux_top = aux_top.groupby(['A | P&S Launch List Unique ID'], as_index=False).agg({\"Region Code\":lambda x: \",\".join(x.dropna().astype(str))})\n",
    "\n",
    "aux_top.drop_duplicates(subset=\"A | P&S Launch List Unique ID\", inplace=True)\n",
    "aux_top.rename(columns={'Region Code':\"Region_100\"}, inplace=True)\n",
    "new_main_df_cols = new_main_df_cols.merge(aux_top, on=\"A | P&S Launch List Unique ID\", how=\"left\")\n",
    "new_main_df_cols['P&S | Top 100 Regions Summary'] = new_main_df_cols['Region_100']\n",
    "new_main_df_cols.drop(columns='Region_100', inplace=True)\n",
    "\n",
    "#new_main_df_cols.drop(columns=\"P&S | Content Pipeline\", inplace=True)\n",
    "new_main_df_cols.drop(columns=['A | P&S Secondary Max Brand','MAX Secondary Brand/s','A | P&S Primary Max Brand','Primary Genre 1 üé≠','Primary Genre 2 üé≠',\"‚úèÔ∏è Planned Premiere Date\",\"P&S | Planned Premiere Date\"], inplace=True)\n",
    "\n",
    "new_main_df_cols['P&S | Greece Converged Scheduling Priority'] = new_main_df_cols['P&S | Greece Converged Scheduling Priority'].str.encode(\"ascii\", \"ignore\").str.decode(\"ascii\")\n",
    "\n",
    "greece_aux = bridge_df[['A | P&S Launch List Unique ID','Greece Vodafone Status']].drop_duplicates(subset='A | P&S Launch List Unique ID').copy()\n",
    "new_main_df_cols = new_main_df_cols.merge(greece_aux, on='A | P&S Launch List Unique ID', how=\"left\")\n",
    "\n",
    "new_main_df_cols.rename(columns={\"Greece Vodafone Status\":\"P&S | Greece Vodafone Status\",\"üîí Production Year\":\"P&S | Production Year\"}, inplace=True)\n",
    "\n",
    "for col in [c for c in new_main_df_cols.columns if \"Secondary\" in c]:\n",
    "    new_main_df_cols[col] = new_main_df_cols[col].where(\n",
    "        new_main_df_cols[col].notna(), \n",
    "        other=new_main_df_cols[col]\n",
    "    ).astype(str).str.replace(\"|\", \",\")\n",
    "    \n",
    "new_main_df_cols['P&S | Kids Age Demo Summary'] = new_main_df_cols['P&S | Kids Age Demo Summary'].str.replace(\"|\",\",\", regex=False)\n",
    "\n",
    "new_main_df_cols[\"P&S | Production Year\"] = (\n",
    "    new_main_df_cols[\"P&S | Production Year\"]\n",
    "    .str.replace(r\"[A-Z]{2,3} - <NA>,?\", \"\", regex=True)  # remove patterns\n",
    "    .str.strip(\", \")  # clean up leftover commas/spaces\n",
    ")\n",
    "\n",
    "cols_premiere = [c for c in new_main_df_cols.columns if \"Premiere\" in c]\n",
    "\n",
    "new_main_df_cols[cols_premiere] = new_main_df_cols[cols_premiere].fillna(\"-\")\n",
    "\n",
    "new_main_df_cols[cols_premiere] = new_main_df_cols[cols_premiere].replace(\"nan\", \"-\")\n",
    "\n",
    "col = \"P&S | Launch List Validation ALL Regions\"\n",
    "\n",
    "\n",
    "\n",
    "# Definimos las condiciones\n",
    "cond1 = new_main_df_cols[col].fillna(\"\").str.contains(\"For Launch|Post Launch\", na=False)\n",
    "cond2 = (\n",
    "    new_main_df_cols[col].fillna(\"\").str.contains(\"No Transition to Max\", na=False)\n",
    "    & ~cond1\n",
    ")\n",
    "cond3 = new_main_df_cols[col].isna() | (new_main_df_cols[col].str.strip() == \"\")\n",
    "\n",
    "# Definimos los valores que asigna cada condici√≥n\n",
    "choices = [\n",
    "    \"Season Active\",        # cond1\n",
    "    \"No Transition to Max\", # cond2\n",
    "    \"Not in Bridge\"         # cond3\n",
    "]\n",
    "\n",
    "# Aplicamos con np.select\n",
    "new_main_df_cols[\"A | Season Active\"] = np.select(\n",
    "    [cond1, cond2, cond3],\n",
    "    choices,\n",
    "    default=\"Season Removed\"  # ELSE final\n",
    ")\n",
    "\n",
    "new_main_df_cols.fillna(\"-\", inplace=True)\n",
    "\n",
    "new_main_df_cols['A | Season/Feature Validation Comments AV'] = new_main_df_cols.apply(compare_history_vs_snapshot_min, axis=1)\n",
    "\n",
    "new_main_df_cols['A | Top 100 Regions Changes Status - Summary'].fillna(\"-\", inplace= True)\n",
    "\n",
    "result.to_excel(output_path / \"bridge_agg.xlsx\", index=False)\n",
    "new_main_df_cols.to_csv(output_path / \"updated_main.csv\", index=False)\n",
    "\n",
    "print(\"Proceso completado ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23918703",
   "metadata": {},
   "source": [
    "## Columna `‚úèÔ∏è Planned Premiere Date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb757ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1e7b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "e2f74f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bridge_path = r\"c:\\Users\\azayas\\Downloads\\W3 Launch List (Rebecca) _ Bridge-GCOA _ Max EMEA W3_W4 _ LL - Ally - 2025-09-18T100855.263 XSLX.xlsx\"\n",
    "\n",
    "pivot_path = r\"C:\\Users\\azayas\\Downloads\\P&S Launch List (Pivot)-EMEA W3 Launch List - Ally (1).csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "2ffff95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azayas\\AppData\\Local\\Temp\\ipykernel_17880\\4016313452.py:2: DtypeWarning: Columns (31,32,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pivot_df = pd.read_csv(pivot_path)\n"
     ]
    }
   ],
   "source": [
    "bridge_df = pd.read_excel(bridge_path, engine='openpyxl')\n",
    "pivot_df = pd.read_csv(pivot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "0f35e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_end_dates(df, col=\"Code + üîí License end\"):\n",
    "    \"\"\"\n",
    "    Expande columna de fechas en columnas separadas conservando la original.\n",
    "    \"\"\"\n",
    "    def extract_dates(text):\n",
    "        if pd.isna(text) or text.strip() == \"-\":\n",
    "            return {}\n",
    "        codes_dict = {}\n",
    "        for block in text.split(\",\"):\n",
    "            #print(block)\n",
    "            block = block.strip()\n",
    "            #print(block)\n",
    "            if \" - \" in block:\n",
    "                code, value = block.split(\" - \", 1)\n",
    "                \n",
    "                codes_dict[f\"A | P&S {code.strip()} {col}\"] = value.strip()\n",
    "        return codes_dict\n",
    "\n",
    "    expanded = df[col].apply(extract_dates).apply(pd.Series)\n",
    "    df = pd.concat([df, expanded], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ea7021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee8d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "6c84017b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A | P&amp;S Launch List Unique ID</th>\n",
       "      <th>‚úèÔ∏è Planned Premiere Date</th>\n",
       "      <th>A | P&amp;S ADR Planned Premiere Date</th>\n",
       "      <th>A | P&amp;S BAL Planned Premiere Date</th>\n",
       "      <th>A | P&amp;S CIS Planned Premiere Date</th>\n",
       "      <th>A | P&amp;S CYP Planned Premiere Date</th>\n",
       "      <th>A | P&amp;S GAU Planned Premiere Date</th>\n",
       "      <th>A | P&amp;S GRE Planned Premiere Date</th>\n",
       "      <th>A | P&amp;S ICE Planned Premiere Date</th>\n",
       "      <th>A | P&amp;S IRE Planned Premiere Date</th>\n",
       "      <th>A | P&amp;S ISR Planned Premiere Date</th>\n",
       "      <th>A | P&amp;S ITA Planned Premiere Date</th>\n",
       "      <th>A | P&amp;S LUX Planned Premiere Date</th>\n",
       "      <th>A | P&amp;S MAL Planned Premiere Date</th>\n",
       "      <th>A | P&amp;S SWL Planned Premiere Date</th>\n",
       "      <th>A | P&amp;S UK Planned Premiere Date</th>\n",
       "      <th>A | P&amp;S UKR Planned Premiere Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P&amp;S-LL-0006195</td>\n",
       "      <td>ADR - nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P&amp;S-LL-0001942</td>\n",
       "      <td>ADR - 14/04/2023,BAL - nan,CIS - nan,CYP - nan...</td>\n",
       "      <td>14/04/2023</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>22/07/2025</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>05/09/2025</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P&amp;S-LL-0000807</td>\n",
       "      <td>CIS - nan,ICE - nan,MAL - nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P&amp;S-LL-0008904</td>\n",
       "      <td>ADR - nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P&amp;S-LL-0002263</td>\n",
       "      <td>ADR - 28/12/2021,CIS - nan,CYP - nan,GAU - nan...</td>\n",
       "      <td>28/12/2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>22/07/2025</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22/07/2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7400</th>\n",
       "      <td>P&amp;S-LL-0003917</td>\n",
       "      <td>CYP - 01/06/2025,GRE - 01/06/2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/06/2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/06/2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7401</th>\n",
       "      <td>P&amp;S-LL-0003969</td>\n",
       "      <td>CYP - nan,GRE - nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7402</th>\n",
       "      <td>P&amp;S-LL-0003944</td>\n",
       "      <td>CYP - nan,GRE - nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7403</th>\n",
       "      <td>P&amp;S-LL-0001405</td>\n",
       "      <td>CYP - 18/07/2025,GRE - 18/07/2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18/07/2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18/07/2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7404</th>\n",
       "      <td>P&amp;S-LL-0004016</td>\n",
       "      <td>CYP - 21/07/2025,GRE - 21/07/2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21/07/2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21/07/2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7405 rows √ó 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     A | P&S Launch List Unique ID  \\\n",
       "0                   P&S-LL-0006195   \n",
       "1                   P&S-LL-0001942   \n",
       "2                   P&S-LL-0000807   \n",
       "3                   P&S-LL-0008904   \n",
       "4                   P&S-LL-0002263   \n",
       "...                            ...   \n",
       "7400                P&S-LL-0003917   \n",
       "7401                P&S-LL-0003969   \n",
       "7402                P&S-LL-0003944   \n",
       "7403                P&S-LL-0001405   \n",
       "7404                P&S-LL-0004016   \n",
       "\n",
       "                               ‚úèÔ∏è Planned Premiere Date  \\\n",
       "0                                             ADR - nan   \n",
       "1     ADR - 14/04/2023,BAL - nan,CIS - nan,CYP - nan...   \n",
       "2                         CIS - nan,ICE - nan,MAL - nan   \n",
       "3                                             ADR - nan   \n",
       "4     ADR - 28/12/2021,CIS - nan,CYP - nan,GAU - nan...   \n",
       "...                                                 ...   \n",
       "7400                  CYP - 01/06/2025,GRE - 01/06/2025   \n",
       "7401                                CYP - nan,GRE - nan   \n",
       "7402                                CYP - nan,GRE - nan   \n",
       "7403                  CYP - 18/07/2025,GRE - 18/07/2025   \n",
       "7404                  CYP - 21/07/2025,GRE - 21/07/2025   \n",
       "\n",
       "     A | P&S ADR Planned Premiere Date A | P&S BAL Planned Premiere Date  \\\n",
       "0                                  nan                               NaN   \n",
       "1                           14/04/2023                               nan   \n",
       "2                                  NaN                               NaN   \n",
       "3                                  nan                               NaN   \n",
       "4                           28/12/2021                               NaN   \n",
       "...                                ...                               ...   \n",
       "7400                               NaN                               NaN   \n",
       "7401                               NaN                               NaN   \n",
       "7402                               NaN                               NaN   \n",
       "7403                               NaN                               NaN   \n",
       "7404                               NaN                               NaN   \n",
       "\n",
       "     A | P&S CIS Planned Premiere Date A | P&S CYP Planned Premiere Date  \\\n",
       "0                                  NaN                               NaN   \n",
       "1                                  nan                               nan   \n",
       "2                                  nan                               NaN   \n",
       "3                                  NaN                               NaN   \n",
       "4                                  nan                               nan   \n",
       "...                                ...                               ...   \n",
       "7400                               NaN                        01/06/2025   \n",
       "7401                               NaN                               nan   \n",
       "7402                               NaN                               nan   \n",
       "7403                               NaN                        18/07/2025   \n",
       "7404                               NaN                        21/07/2025   \n",
       "\n",
       "     A | P&S GAU Planned Premiere Date A | P&S GRE Planned Premiere Date  \\\n",
       "0                                  NaN                               NaN   \n",
       "1                                  nan                               nan   \n",
       "2                                  NaN                               NaN   \n",
       "3                                  NaN                               NaN   \n",
       "4                                  nan                               nan   \n",
       "...                                ...                               ...   \n",
       "7400                               NaN                        01/06/2025   \n",
       "7401                               NaN                               nan   \n",
       "7402                               NaN                               nan   \n",
       "7403                               NaN                        18/07/2025   \n",
       "7404                               NaN                        21/07/2025   \n",
       "\n",
       "     A | P&S ICE Planned Premiere Date A | P&S IRE Planned Premiere Date  \\\n",
       "0                                  NaN                               NaN   \n",
       "1                           22/07/2025                               nan   \n",
       "2                                  nan                               NaN   \n",
       "3                                  NaN                               NaN   \n",
       "4                           22/07/2025                               nan   \n",
       "...                                ...                               ...   \n",
       "7400                               NaN                               NaN   \n",
       "7401                               NaN                               NaN   \n",
       "7402                               NaN                               NaN   \n",
       "7403                               NaN                               NaN   \n",
       "7404                               NaN                               NaN   \n",
       "\n",
       "     A | P&S ISR Planned Premiere Date A | P&S ITA Planned Premiere Date  \\\n",
       "0                                  NaN                               NaN   \n",
       "1                                  nan                               nan   \n",
       "2                                  NaN                               NaN   \n",
       "3                                  NaN                               NaN   \n",
       "4                                  nan                               nan   \n",
       "...                                ...                               ...   \n",
       "7400                               NaN                               NaN   \n",
       "7401                               NaN                               NaN   \n",
       "7402                               NaN                               NaN   \n",
       "7403                               NaN                               NaN   \n",
       "7404                               NaN                               NaN   \n",
       "\n",
       "     A | P&S LUX Planned Premiere Date A | P&S MAL Planned Premiere Date  \\\n",
       "0                                  NaN                               NaN   \n",
       "1                                  nan                        05/09/2025   \n",
       "2                                  NaN                               nan   \n",
       "3                                  NaN                               NaN   \n",
       "4                                  NaN                        22/07/2025   \n",
       "...                                ...                               ...   \n",
       "7400                               NaN                               NaN   \n",
       "7401                               NaN                               NaN   \n",
       "7402                               NaN                               NaN   \n",
       "7403                               NaN                               NaN   \n",
       "7404                               NaN                               NaN   \n",
       "\n",
       "     A | P&S SWL Planned Premiere Date A | P&S UK Planned Premiere Date  \\\n",
       "0                                  NaN                              NaN   \n",
       "1                                  nan                              nan   \n",
       "2                                  NaN                              NaN   \n",
       "3                                  NaN                              NaN   \n",
       "4                                  NaN                              nan   \n",
       "...                                ...                              ...   \n",
       "7400                               NaN                              NaN   \n",
       "7401                               NaN                              NaN   \n",
       "7402                               NaN                              NaN   \n",
       "7403                               NaN                              NaN   \n",
       "7404                               NaN                              NaN   \n",
       "\n",
       "     A | P&S UKR Planned Premiere Date  \n",
       "0                                  NaN  \n",
       "1                                  nan  \n",
       "2                                  NaN  \n",
       "3                                  NaN  \n",
       "4                                  nan  \n",
       "...                                ...  \n",
       "7400                               NaN  \n",
       "7401                               NaN  \n",
       "7402                               NaN  \n",
       "7403                               NaN  \n",
       "7404                               NaN  \n",
       "\n",
       "[7405 rows x 17 columns]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "una_cosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94505d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11d1dc7c",
   "metadata": {},
   "source": [
    "## `P&S Launch List tab` - ``P&S | Production Year``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "8ea9f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_main_df_cols['üîí Production Year'].value_counts().reset_index().to_clipboard(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c97f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_main_df_cols[\"üîí Production Year\"] = (\n",
    "    new_main_df_cols[\"produc\"]\n",
    "    .str.replace(r\"[A-Z]{2,3} - <NA>,?\", \"\", regex=True)  # remove patterns\n",
    "    .str.strip(\", \")  # clean up leftover commas/spaces\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34176983",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_main_df_cols['üîí Production Year'].str.replace(r\"[A-Z]{2,3} - <NA>,?\", \"\", regex=True).str.strip(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498caf24",
   "metadata": {},
   "source": [
    "## ``P&S | Distributor``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "219bcca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SEP23'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc8f5955",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"P&S | Launch List Validation ALL Regions\"\n",
    "\n",
    "\n",
    "\n",
    "# Definimos las condiciones\n",
    "cond1 = new_main_df_cols[col].fillna(\"\").str.contains(\"For Launch|Post Launch\", na=False)\n",
    "cond2 = (\n",
    "    new_main_df_cols[col].fillna(\"\").str.contains(\"No Transition to Max\", na=False)\n",
    "    & ~cond1\n",
    ")\n",
    "cond3 = new_main_df_cols[col].isna() | (new_main_df_cols[col].str.strip() == \"\")\n",
    "\n",
    "# Definimos los valores que asigna cada condici√≥n\n",
    "choices = [\n",
    "    \"Season Active\",        # cond1\n",
    "    \"No Transition to Max\", # cond2\n",
    "    \"Not in Bridge\"         # cond3\n",
    "]\n",
    "\n",
    "# Aplicamos con np.select\n",
    "new_main_df_cols[\"A | Season Active\"] = np.select(\n",
    "    [cond1, cond2, cond3],\n",
    "    choices,\n",
    "    default=\"Season Removed\"  # ELSE final\n",
    ")\n",
    "\n",
    "new_main_df_cols.fillna(\"-\", inplace= True)\n",
    "\n",
    "new_main_df_cols['A | Season/Feature Validation Comments AV'] = new_main_df_cols.apply(compare_history_vs_snapshot_min, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7206c0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       SEP17 - Season Active | SEP01 - Season Active ...\n",
       "1       SEP17 - Season Removed | SEP01 - Season Remove...\n",
       "2       SEP17 - Not in Bridge | SEP01 - Not in Bridge ...\n",
       "3       SEP17 - Not in Bridge | SEP01 - Not in Bridge ...\n",
       "4       SEP17 - Season Active | SEP01 - Season Active ...\n",
       "                              ...                        \n",
       "8675                                                    -\n",
       "8676                                                    -\n",
       "8677                                                    -\n",
       "8678                                                    -\n",
       "8679                                                    -\n",
       "Name: A | Season/Feature Validation Comments AV, Length: 8680, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_main_df_cols['A | Season/Feature Validation Comments AV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "122b10e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SEP23 - Season Active | SEP17 - Season Active | SEP01 - Season Active | AUG13 - Season Active | JUL15 - Season Active | JUL01 - Season Active | JUN09 - Season Active | JUN06 - Season Active | MAY30 - Season Active | MAY26 - Season Active | MAY19-Reviewed - Season Active    4275\n",
       "SEP23 - Season Active | SEP17 - Season Active | SEP01 - Season Active | AUG13 - Season Active | JUL15 - Season Active | JUL01 - Season Active | JUN09 - Season Active | JUN06 - Season Active | MAY30 - Season Active | MAY26 - Season Active | MAY19 - To be reviewed             923\n",
       "SEP23 - Season Removed | SEP17 - Not in Bridge | SEP01 - Not in Bridge | AUG13 - Not in Bridge | JUL15 - Not in Bridge | JUL01 - Not in Bridge | JUN09 - Not in Bridge | JUN06 - Not in Bridge | MAY30 - Not in Bridge | MAY26 - Not in Bridge | MAY19 - To be reviewed            585\n",
       "SEP23 - Removed | SEP17 - Season Removed | SEP01 - Season Removed | AUG13 - Season Removed | JUL15 - Season Removed | JUL01 - Season Removed | JUN09 - Season Removed | JUN06 - Season Removed | MAY30 - Season Removed | MAY26 - Removed from Launch                              351\n",
       "SEP23 - Season Active | SEP17 - Season Active | SEP01 - Season Active | AUG13 - Season Active | JUL15 - Season Active                                                                                                                                                              279\n",
       "                                                                                                                                                                                                                                                                                  ... \n",
       "SEP23 - Season Removed | SEP17 - Not in Bridge | SEP01 - Season Active | AUG13 - Season Active                                                                                                                                                                                       1\n",
       "SEP23 - Season Removed | SEP17 - Not in Bridge | SEP01 - Season Removed | AUG13 - Season Removed                                                                                                                                                                                     1\n",
       "SEP23 - Season Active | SEP17 - Season Active | SEP01 - Season Active | AUG21 - Season Active                                                                                                                                                                                        1\n",
       "SEP23 - Season Removed | SEP17 - Not in Bridge                                                                                                                                                                                                                                       1\n",
       "SEP23 - Season Active | SEP17 - No Transition to Max | SEP01 - No Transition to Max                                                                                                                                                                                                  1\n",
       "Name: count, Length: 128, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_history_vs_snapshot_min(row): \n",
    "    history = row['A | Season/Feature Validation Comments AV'] \n",
    "    snapshot = row['A | Season Active'] \n",
    "    # Los convertimos a lista \n",
    "    hist_entries = [h.strip() for h in history.split(\",\") if h.strip() !=\"\"] \n",
    "    snap_entries = [s.strip() for s in snapshot.split(\",\") if s.strip()!=\"\"] \n",
    "    changes = [] \n",
    "    \n",
    "    for snap in snap_entries: \n",
    "        if snap.__contains__(\"No Transition\"): \n",
    "            changes.append(f\"{today} - Removed\") \n",
    "        else: \n",
    "            changes.append(F\"{today} - {snap}\") \n",
    "    \n",
    "    results = changes + hist_entries \n",
    "    \n",
    "    return \" | \".join(results)\n",
    "\n",
    "\n",
    "\n",
    "new_main_df_cols.apply(compare_history_vs_snapshot_min, axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db8b239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2188162b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
